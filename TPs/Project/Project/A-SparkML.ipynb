{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/29 12:59:41 WARN Utils: Your hostname, TABLET-UD6BNBK5 resolves to a loopback address: 127.0.1.1; using 172.30.100.210 instead (on interface eth0)\n",
      "22/01/29 12:59:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/aympab/local/anaconda3/envs/hadoop-spark/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/29 12:59:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"----------------------------------------------------------------------------\n",
    "CREATE SPARK CONTEXT\n",
    "CREATE SQL CONTEXT\n",
    "----------------------------------------------------------------------------\"\"\"\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      " |-- ind_variety: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(variety='Virginica', ind_variety=2.0),\n",
       " Row(variety='Versicolor', ind_variety=1.0),\n",
       " Row(variety='Setosa', ind_variety=0.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"----------------------------------------------------------------------------\n",
    "LOAD IRIS DATA\n",
    "----------------------------------------------------------------------------\"\"\"\n",
    "data_dir=\"\"\n",
    "file = os.path.join(data_dir,\"iris.csv\")\n",
    "panda_df = pd.read_csv(file)\n",
    "\n",
    "iris_df=sqlContext.createDataFrame(panda_df)\n",
    "iris_df.printSchema()\n",
    "\n",
    "#Add a numeric indexer for the label/target column\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"variety\", outputCol=\"ind_variety\")\n",
    "si_model = stringIndexer.fit(iris_df)\n",
    "irisNormDf = si_model.transform(iris_df)\n",
    "irisNormDf.printSchema()\n",
    "irisNormDf.select(\"variety\",\"ind_variety\").distinct().collect()\n",
    "#irisNormDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|summary|      sepal_length|       sepal_width|      petal_length|       petal_width|  variety|       ind_variety|\n",
      "+-------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|  count|               150|               150|               150|               150|      150|               150|\n",
      "|   mean| 5.843333333333334|3.0573333333333332|3.7580000000000005|1.1993333333333331|     null|               1.0|\n",
      "| stddev|0.8280661279778632|0.4358662849366984| 1.765298233259466|0.7622376689603464|     null|0.8192319205190405|\n",
      "|    min|               4.3|               2.0|               1.0|               0.1|   Setosa|               0.0|\n",
      "|    max|               7.9|               4.4|               6.9|               2.5|Virginica|               2.0|\n",
      "+-------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Data Analytics\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#See standard parameters\n",
    "irisNormDf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+\n",
      "|species|label|         features|\n",
      "+-------+-----+-----------------+\n",
      "| Setosa|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "| Setosa|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "| Setosa|  0.0|[5.4,3.9,1.7,0.4]|\n",
      "| Setosa|  0.0|[4.6,3.4,1.4,0.3]|\n",
      "| Setosa|  0.0|[5.0,3.4,1.5,0.2]|\n",
      "| Setosa|  0.0|[4.4,2.9,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.1,1.5,0.1]|\n",
      "| Setosa|  0.0|[5.4,3.7,1.5,0.2]|\n",
      "| Setosa|  0.0|[4.8,3.4,1.6,0.2]|\n",
      "| Setosa|  0.0|[4.8,3.0,1.4,0.1]|\n",
      "| Setosa|  0.0|[4.3,3.0,1.1,0.1]|\n",
      "| Setosa|  0.0|[5.8,4.0,1.2,0.2]|\n",
      "| Setosa|  0.0|[5.7,4.4,1.5,0.4]|\n",
      "| Setosa|  0.0|[5.4,3.9,1.3,0.4]|\n",
      "| Setosa|  0.0|[5.1,3.5,1.4,0.3]|\n",
      "| Setosa|  0.0|[5.7,3.8,1.7,0.3]|\n",
      "| Setosa|  0.0|[5.1,3.8,1.5,0.3]|\n",
      "| Setosa|  0.0|[5.4,3.4,1.7,0.2]|\n",
      "| Setosa|  0.0|[5.1,3.7,1.5,0.4]|\n",
      "| Setosa|  0.0|[4.6,3.6,1.0,0.2]|\n",
      "| Setosa|  0.0|[5.1,3.3,1.7,0.5]|\n",
      "| Setosa|  0.0|[4.8,3.4,1.9,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.0,1.6,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.4,1.6,0.4]|\n",
      "| Setosa|  0.0|[5.2,3.5,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.2,3.4,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.7,3.2,1.6,0.2]|\n",
      "| Setosa|  0.0|[4.8,3.1,1.6,0.2]|\n",
      "| Setosa|  0.0|[5.4,3.4,1.5,0.4]|\n",
      "| Setosa|  0.0|[5.2,4.1,1.5,0.1]|\n",
      "| Setosa|  0.0|[5.5,4.2,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.1,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.2,1.2,0.2]|\n",
      "| Setosa|  0.0|[5.5,3.5,1.3,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.6,1.4,0.1]|\n",
      "| Setosa|  0.0|[4.4,3.0,1.3,0.2]|\n",
      "| Setosa|  0.0|[5.1,3.4,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.5,1.3,0.3]|\n",
      "| Setosa|  0.0|[4.5,2.3,1.3,0.3]|\n",
      "| Setosa|  0.0|[4.4,3.2,1.3,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.5,1.6,0.6]|\n",
      "| Setosa|  0.0|[5.1,3.8,1.9,0.4]|\n",
      "| Setosa|  0.0|[4.8,3.0,1.4,0.3]|\n",
      "| Setosa|  0.0|[5.1,3.8,1.6,0.2]|\n",
      "| Setosa|  0.0|[4.6,3.2,1.4,0.2]|\n",
      "| Setosa|  0.0|[5.3,3.7,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.3,1.4,0.2]|\n",
      "+-------+-----+-----------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[species: string, label: double, features: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Prepare data for ML\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#Transform to a Data Frame for input to Machine Learing\n",
    "#Drop columns that are not required (low correlation)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "def transformToLabeledPoint(row) :\n",
    "    lp = ( row[\"variety\"], row[\"ind_variety\"], \\\n",
    "                Vectors.dense([row[\"sepal_length\"],\\\n",
    "                        row[\"sepal_width\"], \\\n",
    "                        row[\"petal_length\"], \\\n",
    "                        row[\"petal_width\"]]))\n",
    "    return lp\n",
    "\n",
    "irisLp = irisNormDf.rdd.map(transformToLabeledPoint)\n",
    "irisLpDf = sqlContext.createDataFrame(irisLp,[\"species\",\"label\", \"features\"])\n",
    "irisLpDf.select(\"species\",\"label\",\"features\").show(50)\n",
    "irisLpDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(species='Setosa', label=0.0, features=DenseVector([4.8, 3.4, 1.9, 0.2])),\n",
       " Row(species='Setosa', label=0.0, features=DenseVector([5.2, 3.5, 1.5, 0.2])),\n",
       " Row(species='Setosa', label=0.0, features=DenseVector([4.8, 3.0, 1.4, 0.3])),\n",
       " Row(species='Setosa', label=0.0, features=DenseVector([4.9, 3.6, 1.4, 0.1])),\n",
       " Row(species='Setosa', label=0.0, features=DenseVector([5.1, 3.8, 1.6, 0.2])),\n",
       " Row(species='Setosa', label=0.0, features=DenseVector([5.1, 3.8, 1.9, 0.4])),\n",
       " Row(species='Setosa', label=0.0, features=DenseVector([5.3, 3.7, 1.5, 0.2])),\n",
       " Row(species='Versicolor', label=1.0, features=DenseVector([7.0, 3.2, 4.7, 1.4])),\n",
       " Row(species='Versicolor', label=1.0, features=DenseVector([5.2, 2.7, 3.9, 1.4])),\n",
       " Row(species='Versicolor', label=1.0, features=DenseVector([5.7, 2.8, 4.5, 1.3])),\n",
       " Row(species='Versicolor', label=1.0, features=DenseVector([6.6, 2.9, 4.6, 1.3])),\n",
       " Row(species='Versicolor', label=1.0, features=DenseVector([6.8, 2.8, 4.8, 1.4])),\n",
       " Row(species='Versicolor', label=1.0, features=DenseVector([5.0, 2.3, 3.3, 1.0])),\n",
       " Row(species='Virginica', label=2.0, features=DenseVector([5.6, 2.8, 4.9, 2.0])),\n",
       " Row(species='Virginica', label=2.0, features=DenseVector([7.2, 3.2, 6.0, 1.8])),\n",
       " Row(species='Virginica', label=2.0, features=DenseVector([7.2, 3.0, 5.8, 1.6]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Machine Learning\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#Split into training and testing data\n",
    "(trainingData, testData) = irisLpDf.randomSplit([0.9, 0.1])\n",
    "trainingData.count()\n",
    "testData.count()\n",
    "testData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Create the model\n",
    "dtClassifer = DecisionTreeClassifier(maxDepth=4, labelCol=\"label\",\\\n",
    "                featuresCol=\"features\")\n",
    "dtModel = dtClassifer.fit(trainingData)\n",
    "\n",
    "rfClasifier = RandomForestClassifier(maxDepth=4, labelCol=\"label\",\\\n",
    "                featuresCol=\"features\")\n",
    "rfModel = rfClasifier.fit(trainingData)\n",
    "\n",
    "# gbtClassifier = GBTClassifier(maxDepth=4, labelCol=\"label\",\\\n",
    "#                 featuresCol=\"features\")\n",
    "# gbtModel = gbtClassifier.fit(trainingData)\n",
    "\n",
    "# print(rfModel.numNodes)\n",
    "# print(rfModel.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0.0, species='Setosa', label=0.0),\n",
       " Row(prediction=0.0, species='Setosa', label=0.0),\n",
       " Row(prediction=0.0, species='Setosa', label=0.0),\n",
       " Row(prediction=0.0, species='Setosa', label=0.0),\n",
       " Row(prediction=0.0, species='Setosa', label=0.0),\n",
       " Row(prediction=0.0, species='Setosa', label=0.0),\n",
       " Row(prediction=0.0, species='Setosa', label=0.0),\n",
       " Row(prediction=1.0, species='Versicolor', label=1.0),\n",
       " Row(prediction=1.0, species='Versicolor', label=1.0),\n",
       " Row(prediction=1.0, species='Versicolor', label=1.0),\n",
       " Row(prediction=1.0, species='Versicolor', label=1.0),\n",
       " Row(prediction=1.0, species='Versicolor', label=1.0),\n",
       " Row(prediction=1.0, species='Versicolor', label=1.0),\n",
       " Row(prediction=2.0, species='Virginica', label=2.0),\n",
       " Row(prediction=2.0, species='Virginica', label=2.0),\n",
       " Row(prediction=2.0, species='Virginica', label=2.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict on the test data\n",
    "predictions = dtModel.transform(testData)\n",
    "predictions.select(\"prediction\",\"species\",\"label\").collect()\n",
    "\n",
    "predictions_rf = rfModel.transform(testData)\n",
    "predictions_rf.select(\"prediction\",\"species\",\"label\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f986d021d30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aympab/local/anaconda3/envs/hadoop-spark/lib/python3.9/site-packages/pyspark/ml/wrapper.py\", line 39, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'RandomForestClassifier' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "evaluator.evaluate(predictions)    \n",
    "\n",
    "#Draw a confusion matrix\n",
    "# predictions.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate accuracy\n",
    "evaluator_rf = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "evaluator_rf.evaluate(predictions_rf)    \n",
    "\n",
    "#Draw a confusion matrix\n",
    "# predictions_rf.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e05be29a4969219dd5201c77e77ae89f174b711dfe556cd662effe1dc933b586"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('hadoop-spark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
